import os
from dotenv import load_dotenv
from deepeval import evaluate
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualPrecisionMetric,
    ContextualRecallMetric,
    BiasMetric,
    ToxicityMetric
)
from deepeval.test_case import LLMTestCase
from langchain_openai import ChatOpenAI

# --- 1. Setup ---
# Load environment variables from .env file (for OPENAI_API_KEY)
load_dotenv()

# Check if the OpenAI API key is set
if "OPENAI_API_KEY" not in os.environ:
    print("Error: OPENAI_API_KEY is not set in the environment variables.")
    print("Please create a .env file and add OPENAI_API_KEY='your_key_here'")
    exit()

# Initialize the Language Model from LangChain
# Using gpt-4o as it's strong in reasoning and code understanding.
# You can switch to another model like 'gpt-3.5-turbo' if needed.
try:
    llm = ChatOpenAI(model="gpt-4o")
except Exception as e:
    print(f"Error initializing ChatOpenAI: {e}")
    print("Please ensure your OpenAI API key is valid.")
    exit()


# --- 2. Define Your Data (Redefined Structure) ---
# This section reflects the new Q&A structure of your application.

# Input to your application
user_question = "What is the purpose of the CUSTOMER-REPORT program and what file does it write to?"

# Outputs from your application's retrieval steps
retrieved_metadata = """
{
    "program_id": "CUSTOMER-REPORT",
    "purpose": "To generate a report of customer balances from a sequential data file.",
    "input_file": "CUST.DAT",
    "output_file": "CUST.RPT",
    "schedule": "Runs daily at 5:00 AM."
}
"""

retrieved_documentation = """
Program: CUSTOMER-REPORT
Author: A. WYATT
Description: This batch program reads the CUST.DAT file, which contains customer records (ID, Name, Balance). For each record, it formats a line and writes it to the report file. The program terminates after processing all records. The output is a simple, unformatted list of customer names and balances.
"""

# The final, synthesized answer from your application
final_answer = "The CUSTOMER-REPORT program generates a daily report of customer balances. It writes its output to the 'CUST.RPT' file. The author of the program is B. Miller."


# --- 3. Prepare Inputs for DeepEval ---
# The 'input' is the original question asked by the user.
# The 'actual_output' is the final answer generated by your system.
# The 'retrieval_context' is all the source material your system used to generate the answer.

retrieval_context = [
    retrieved_metadata,
    retrieved_documentation
]

# --- 4. Define the Evaluation Metrics ---
# These metrics are well-suited for a RAG (Retrieval-Augmented Generation) system.
# Note: SummarizationMetric is removed as we are no longer just summarizing.
metrics = [
    # Checks if the answer is relevant to the user's question.
    AnswerRelevancyMetric(threshold=0.8, model=llm),
    # Checks if the answer hallucinates or makes up facts not in the retrieved context.
    # This should catch that the author is wrong in the 'final_answer'.
    FaithfulnessMetric(threshold=0.8, model=llm),
    # Checks if the answer only uses information from the provided context.
    ContextualPrecisionMetric(threshold=0.8, model=llm),
    # Checks if the answer uses all the necessary information from the context to be complete.
    ContextualRecallMetric(threshold=0.8, model=llm),
    # Checks for any language bias (social, political, etc.).
    BiasMetric(threshold=0.5, model=llm),
    # Checks for any toxic or unprofessional language.
    ToxicityMetric(threshold=0.5, model=llm)
]

# --- 5. Create the Test Case ---
# A TestCase bundles the input, output, and context for evaluation.
test_case = LLMTestCase(
    input=user_question,
    actual_output=final_answer,
    retrieval_context=retrieval_context
)

# --- 6. Run the Evaluation ---
print("ðŸš€ Starting evaluation of the generated Q&A answer...")
print("-" * 50)

# The evaluate function runs all metrics against the test case.
evaluation_results = evaluate([test_case], metrics, print_results=False) # We will print custom results

# --- 7. Display Results ---
print("\nâœ… Evaluation Complete. Results:")
print("=" * 50)

for result in evaluation_results:
    print(f"Metric: {result.metrics[0].__class__.__name__}")
    print(f"  Score: {result.metrics[0].score:.2f} / 1.00")
    print(f"  Threshold: {result.metrics[0].threshold}")
    print(f"  Success: {'Yes' if result.success else 'No'}")
    # The reason is the most valuable part, explaining the score.
    print(f"  Reasoning: {result.metrics[0].reason}\n")

print("-" * 50)
print(f"Overall Success: {'Yes' if all(r.success for r in evaluation_results) else 'No'}")
print("=" * 50)

